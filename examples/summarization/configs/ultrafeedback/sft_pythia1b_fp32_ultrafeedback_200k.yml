model_name: EleutherAI/pythia-1b-deduped
dataset_name: HuggingFaceH4/ultrachat_200k
train_split: train_sft
eval_split: test_sft
output_dir: /home/mila/h/hosseinia/scratch/dpo_proj/output/sft-pythia1b-fp32-ultrafeedback-200k-2x4
log_with: "wandb"
learning_rate: 2.0e-04
lr_scheduler_type: cosine
num_warmup_steps: 0
max_steps: -1
warmup_ratio: 0.1
weight_decay: 0.05
load_in_8bit: False
use_peft: False
fp16: False
bf16: False
streaming: False
gradient_checkpointing: False
gradient_accumulation_steps: 4
per_device_train_batch_size: 2
per_device_eval_batch_size: 8
train_completions: False
num_train_epochs: 1
save_steps: 500
seq_length: 2048
