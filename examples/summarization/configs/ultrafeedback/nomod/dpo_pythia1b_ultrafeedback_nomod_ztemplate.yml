model_name: /home/mila/h/hosseinia/scratch/dpo_proj/output/sft-pythia1b-bf16-ultrafeedback-200k_ztemplate/final_model
dataset_name: HuggingFaceH4/ultrafeedback_binarized
train_split: train_prefs
eval_split: test_prefs
# gold_model_name: 
output_dir: /network/scratch/h/hosseinia/dpo_porj/dpo_pythia1b_bf16_from_sft1b_ultrachat200k_ztemplate
gold_dataset_name: HuggingFaceH4/ultrafeedback_binarized
gold_eval_split: test_gen[:10]
cache_dir: /home/mila/h/hosseinia/scratch/dpo_mn_data_cache
beta: 0.01
logging_steps: 10
num_train_epochs: 1
eval_steps: 500
save_steps: 500
eval_first_step: True
load_in_8bit: False
bf16: True
fp16: False
gold_fp16: False
learning_rate: 5.0e-6
lr_scheduler_type: cosine
use_peft: True
lora_all_linear: True
lora_r: 128
lora_alpha: 128
lora_dropout: 0.05
gradient_accumulation_steps: 4
per_device_train_batch_size: 4
gradient_checkpointing: True
optimizer_type: paged_adamw_32bit
warmup_ratio: 0.1
generate_greedy: True
max_length: 1024
max_target_length: 256
