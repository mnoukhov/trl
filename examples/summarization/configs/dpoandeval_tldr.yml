task_type: "tldr"
# hub_model_id: "mnoukhov/pythia410m-dpo-tldr-full"
push_to_hub: False
model_name: "mnoukhov/pythia410m-sft-tldr"
dataset_name: mnoukhov/summarize_from_feedback_oai_preprocessing_1706381144_relabel_pythia1b
dataset_eval_split: validation
learning_rate: 1e-6
lr_scheduler_type: cosine
fp16: True
gradient_accumulation_steps: 4
per_device_train_batch_size: 4
per_device_eval_batch_size: 8
num_train_epochs: 1
max_length: 640
max_prompt_length: 512
max_target_length: 128
beta: 0.01
## peft
loss_type: ipo
use_peft: False
# lora_r: 16
# lora_alpha: 32
# lora_dropout: 0.
gradient_checkpointing: False
## save strategy
evaluation_strategy: "steps"
eval_steps: 0.2
save_strategy: steps
save_steps: 0.2
hub_strategy: all_checkpoints
logging_steps: 100
ddp_find_unused_parameters: False
# 
#
generate_dataset_name: vwxyzjn/summarize_from_feedback_tldr_3_filtered_oai_preprocessing_1706381144
gold_model_name: cleanrl/EleutherAI_pythia-1b-deduped__reward__tldr
gold_tokenizer_name: EleutherAI/pythia-1b-deduped
max_new_tokens: 128
temperature: 0.010001
generate_batch_size: 16
eval_batch_size: 8
generated_output_name: _generated_dataset
wandb_log_id: model_path
